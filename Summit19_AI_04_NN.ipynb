{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/insight.svg\" style=\"width: 300px\"><br>\n",
    "<font color='#544640'>\n",
    "<center><i>Engineering Summit 2019</i></center>\n",
    "<center><i>Denver, Colorado</i></center></font><br>\n",
    "<center><i><font color='#544640' size='1'>Author: Victor Aranda</font></i></center></font>\n",
    "<center><i><font color='#B81590' size='1'>victor.aranda@insight.com</font></i></center></font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#D21087\">Artificial Neural Networks</font>\n",
    "\n",
    "<font color='#544640'>There are fewer things hotter than ***neural networks*** and ***deep-learning*** these days, although how much of that is just the hype cycle at work is yet to be determined. It would seem that this trend is not just another flash-in-the-pan innovation. Machine Learning, powered by modern compute infrastructure, has become something on the order of the transistor in terms of how much it can potentially change our world.\n",
    "\n",
    ">A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of real biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be âˆ’1 and 1.<br><br>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Neural_network<br><br>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Artificial_neural_network<br><br>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>The principles of Artificial Neural Networks and Artificial Neurons are inspired directly by biology:<br><br>\n",
    "\n",
    "<center><img src='./img/neuron_anatomy.png' width=600></center>\n",
    "<center>Anatomy of a biological neuron</center>\n",
    "<center><font size=1>Image credit: Wikipedia</font></center><br><br>\n",
    "\n",
    "\n",
    "<center><img src='./img/neurons1.gif' width=600></center>\n",
    "<center>Artist's conception</center><br><br>\n",
    "\n",
    "<center><img src='./img/neurons2.gif' width=600></center>\n",
    "<center>Conceptual representation</center><br><br>\n",
    "\n",
    "\n",
    "<center><img src='./img/neurons3.gif' width=600></center>\n",
    "<center>Actual mouse neurons communicating with each other<br><font size=1>Rockefeller University; <i>Fast volumetric calcium imaging across<br> multiple cortical layers using sculpted light</i>, Nature Methods<br><a href='http://dx.doi.org/10.1038/nmeth.4040'>DOI: 10.1038/nmeth.4040</a></font></center><br><br>\n",
    "\n",
    "\n",
    "There are many thousands of books and articles written on this topic - go explore!\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#D21087\">Perceptron</font>\n",
    "\n",
    "<font color='#544640'>The concept of a self-updating ***perceptron***, which is just a single artificial neuron, has been around for a long time, first arising theoretically in 1873 and later developed by computer scientists and psychologists in the 1940s and 1950s.\n",
    "\n",
    "> In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Perceptron<br><br>\n",
    "\n",
    "How does it work? An perceptron looks like this:\n",
    "\n",
    "\n",
    "It has inputs, an ***activation function*** and an output. That's it! We feed a training data set to the perceptron. After each data point is sent throughj the perceptron, the output is evaluated to see whether it is correct. A single neuron (perceptron) serves as a ***linear discriminator*** - it forms a boundary line between two different groups within the data, and will eventually converge if there is a line that can segment those groups of data points.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='#544640'>Image credits:\n",
    "\n",
    "https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d<br><br>\n",
    "\n",
    "<center><img src='./img/perceptron3.png' width=600 alt='Perceptron'><br></center>\n",
    "<center>Basic Structure</center><br><br>\n",
    "\n",
    "<center><img src='./img/perceptron4.png' width=600 alt='Perceptron'><br></center>\n",
    "<center>Output - $y$ - depends on the inputs $x_i$ and the weights $w$. We initialize the weights randomly.</center><br><br>\n",
    "\n",
    "<center><img src='./img/perceptron5.png' width=600 alt='Perceptron'><br></center>\n",
    "<center>Technically this is more accurate - there is an extra term called the ***bias*** ($x_0 = 1, w_0$) at each layer.</center><br><br>\n",
    "\n",
    "<center><img src='./img/perceptron6.png' width=600 alt='Perceptron'><br></center>\n",
    "<center>What could the inputs be?</center><br><br>\n",
    "\n",
    "<center><img src='./img/perceptron7.png' width=400 alt='Perceptron'><br></center>\n",
    "<center>Pseudocode algorithm for updating the perceptron weights</center><br><br>\n",
    "\n",
    "Here is what the convergence process looks like as points are fed into the perceptron and its weights are updated in each iteration:<br><br>\n",
    "\n",
    "<center><img src='./img/perceptron2.png' width=500 alt='Perceptron'></center>\n",
    "<center><font size=1>By <a href='https://commons.wikimedia.org/w/index.php?curid=40188333'>By Elizabeth Goodspeed - Own work, CC BY-SA 4.0</a> - Own work, CC BY-SA 3.0 </font></center><br><br><br>\n",
    "\n",
    "<center><img src='./img/perceptron1.gif' width=400 alt='Perceptron'><br></center><br><br>\n",
    "\n",
    "The activation function does not have to be a simple step function. For example:\n",
    "\n",
    "<center><img src='./img/activation_func.png' width=600><br></center><br><br>\n",
    "\n",
    "There are several popular activation functions that have different characteristics - we won't get into the nitty gritty details.<br><br>\n",
    "\n",
    "<center><img src='./img/activation_func2.png' width=400><br></center><br><br>\n",
    "\n",
    "Credit: \n",
    "http://orngunnarsson.blogspot.com/2017/04/activation-functions-tanh-vs-sigmoid.html\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>Makes sense?</font><br><br>\n",
    "\n",
    "<center><img src='./img/dont_believe.gif' width=400><br></center><br>\n",
    "\n",
    "<font color='#544640'>Ok! Let's build one from scratch.</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./data/ABy.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "\n",
    "def abline(slope, intercept, linestyle, a, w, c='b'):\n",
    "    axes = plt.gca()\n",
    "    x = np.array(axes.get_xlim())\n",
    "    y = intercept + slope * x\n",
    "    plt.plot(x, y, linestyle, alpha=a, linewidth=w, color=c)\n",
    "\n",
    "def activate(x):\n",
    "    return 1 if x >= 0 else -1\n",
    "\n",
    "def predict(w, x):\n",
    "    return activate(np.dot(w, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set initial values\n",
    "eta = 0.1\n",
    "epoch = 1\n",
    "debug = False\n",
    "converged = False\n",
    "w = np.array([np.random.randint(-5,5), \n",
    "              np.random.randint(-5,5),\n",
    "              np.random.randint(-5,5)])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "ax.set_xlim(0,6)\n",
    "ax.set_title(\"Two Populations\")\n",
    "ax.set_xlabel('A')\n",
    "ax.set_ylabel('B')\n",
    "ax.set_xlim(0,5)\n",
    "ax.set_ylim(0,6)\n",
    "\n",
    "print('Initial: w = ' + str([i for i in w]))\n",
    "\n",
    "while not converged:\n",
    "    converged = True\n",
    "    for i in df2.index:\n",
    "        X = np.insert(np.array(df2.iloc[i][0:2]),0,1)\n",
    "        y = np.array(df2.iloc[i][2])\n",
    "        if debug:\n",
    "            print('\\nEpoch ' + str(epoch) + ', point ' + str(i) + \n",
    "                  '\\n   X: ' + str(X) + \n",
    "                  '\\n   w: ' + str(w) +\n",
    "                  '\\n   y: ' + str(y))\n",
    "        if not y == predict(w, X):\n",
    "            converged = False\n",
    "            temp_x = -1*w[1]/w[2]\n",
    "            temp_b = -1*w[0]/w[2]\n",
    "            abline(temp_x, temp_b,'--',.5,.5,'silver')\n",
    "            w = w + y * eta * X\n",
    "        else:\n",
    "            if debug:\n",
    "                print('   No update for this point.')\n",
    "    epoch = epoch + 1\n",
    "\n",
    "print('Converged: w = ' + str([round(i,2) for i in w]))\n",
    "print('Converged in epoch # ' + str(epoch) + ' after point # ' + str(i) + '.')\n",
    "\n",
    "x = -1*w[1]/w[2]\n",
    "b = -1*w[0]/w[2]\n",
    "abline(x, b,'--b',1,1)\n",
    "\n",
    "df2_a = df2.loc[df2.y < 0]\n",
    "df2_b = df2.loc[df2.y > 0]\n",
    "\n",
    "ax.scatter(x=df2_a.A, y=df2_a.B, c='g', label='-1')\n",
    "ax.scatter(x=df2_b.A, y=df2_b.B, c='r', label='1')\n",
    "\n",
    "line_eq = '$x_2 = ' + str(round(x,3)) + 'x_1 + ' + str(round(b,3)) + '$ (decision boundary)'\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles.append(Line2D([0], [0], color='b', linestyle='--', lw=1, label=line_eq))\n",
    "plt.legend(handles=handles)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>We finally arrive at a ***decision boundary*** which separates the two groups of data points. <br><br>\n",
    "    \n",
    "<center><img src='./img/every_time.gif' width=400><br></center><br><br><br>\n",
    "\n",
    "<center>Makes sense now?</center></font><br><br><br>\n",
    "\n",
    "<center><img src='./img/sense.gif' width=400><br></center><br><br>\n",
    "\n",
    "<font color='#544640'>Run the next cell a few times (it will be different every time). Can you tell which cases would be linearly separable? How would you deal with the cases that were not linearly separable?</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "\n",
    "offsetX1 = np.random.randint(-10,10)\n",
    "offsetY1 = np.random.randint(-10,10)\n",
    "offsetX2 = np.random.randint(-10,10)\n",
    "offsetY2 = np.random.randint(-10,10)\n",
    "\n",
    "spreadX1 = np.random.random()\n",
    "spreadY1 = np.random.random()\n",
    "\n",
    "spreadX2 = np.random.random()\n",
    "spreadY2 = np.random.random()\n",
    "\n",
    "X1 = np.random.normal(offsetX1,1,n) * spreadX1\n",
    "Y1 = np.random.normal(offsetY1,1,n) * spreadY1\n",
    "\n",
    "X2 = np.random.normal(offsetX2,1,n) * spreadX2\n",
    "Y2 = np.random.normal(offsetY2,1,n) * spreadY2\n",
    "\n",
    "\n",
    "plt.scatter(X1,Y1)\n",
    "plt.scatter(X2,Y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#D21087\">Multi-Layer Neural Networks</font>\n",
    "\n",
    "<font color='#544640'>How did you answer the previous question about linear separability?\n",
    "    \n",
    "Now that we are <b>Masters of the Perceptron</b>, we can begin exploring neural networks, which are combinations of neurons chained together to achieve a specific result.\n",
    "\n",
    "Because we are just beginning to explore NNs, development and innovation is going on every day and novel papers and techniques are uncovered and published with regularity. It was not until 2012 that a deep neural network, AlexNet, shattered records in a handwritten digit recognition competition, which is one of the precipitating events behind the explosion of Convolutional Neural Networks (CNN) and Deep Neural Networks (DNN).\n",
    "\n",
    "Let's keep things simple. How would we build a classifier for the following distribution of two groups (red and green) in the following data points?<br><br>\n",
    "\n",
    "<center><img src='./img/nonlinear_boundary.png' width=400><br></center><br><br>\n",
    "\n",
    "By combining more than one perceptron, we can produce a non-linear decision boundary.<br><br>\n",
    "\n",
    "\n",
    "<center><img src='./img/multiple_perceptron1.png' width=200><br></center><br><br>\n",
    "\n",
    "<center><img src='./img/multiple_perceptron2.png' width=200><br></center><br><br>\n",
    "\n",
    "Things just get wild from there! What do you think a network like this is capable of?<br><br>\n",
    "\n",
    "<center><img src='./img/multilayer_nn.png' width=600><br></center><br><br>\n",
    "\n",
    "How about this?<br><br>\n",
    "\n",
    "<center><img src='./img/deep_nn.png' width=600><br></center><br><br>\n",
    "\n",
    "Here is a simplified illustration of different flavors of Neural Networks comprised of varying types of cells, activation functions, layers, and topologies.<br><br>\n",
    "\n",
    "Credit: https://www.asimovinstitute.org/author/fjodorvanveen/<br><br>\n",
    "\n",
    "<center><img src='./img/nn_types.png' width=800><br></center><br><br>\n",
    "\n",
    "<center><img src='./img/nn_graphs.png' width=1000><br></center><br><br>\n",
    "\n",
    "These networks take a tremendous amount of data and compute resources to train. How much?\n",
    "\n",
    "A tremendous amount. But the achievements of NN-driven machine learning are amazing. Only in recent years has this been possible - due to improvements to compute, data handling, storage, and thanks the development of the industry as a whole, which tends towards publishing and open-source projects.<br><br>\n",
    "\n",
    "<center><img src='./img/training_data.png' width=400><br></center><br><br>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
