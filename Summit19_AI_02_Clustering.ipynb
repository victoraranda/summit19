{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/insight.svg\" style=\"width: 300px\"><br>\n",
    "<font color='#544640'>\n",
    "<center><i>Engineering Summit 2019</i></center>\n",
    "<center><i>Denver, Colorado</i></center></font><br>\n",
    "<center><i><font color='#544640' size='1'>Author: Victor Aranda</font></i></center></font>\n",
    "<center><i><font color='#B81590' size='1'>victor.aranda@insight.com</font></i></center></font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#D21087\">Clustering & Classification</font>\n",
    "\n",
    "<font color='#544640'>\n",
    "\n",
    "> Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.\n",
    "\n",
    "> Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired values.\n",
    "\n",
    "<br>https://en.wikipedia.org/wiki/Cluster_analysis<br><br><br>\n",
    "\n",
    "\n",
    "<center><img src='./img/clusters.gif' width=600 alt='KNN'></center><br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#D21087\">$k$-Nearest Neighbors Clustering</font>\n",
    "\n",
    "<font color='#544640'>\n",
    "\n",
    ">In pattern recognition, the $k$-nearest neighbors algorithm ($k$-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the $k$ closest training examples in the feature space.\n",
    "\n",
    "k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\n",
    "\n",
    "<center><img src='./img/knn1.gif' width=400 alt='KNN'><br></center>\n",
    "\n",
    "Both for classification and regression, a useful technique can be used to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of $1/d$, where $d$ is the distance to the neighbor.\n",
    "\n",
    "The neighbors are taken from a set of objects for which the class (for $k$-NN classification) or the object property value (for $k$-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\n",
    "\n",
    "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm<br><br><br>\n",
    "\n",
    "\n",
    "<center><img src='./img/knn.png' width=300 alt='k Nearest Neighbors'><br></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>In this example we'll be using some simulated healthcare data. The data files for training and test are named `healthcareTrain.csv` and `healthcareTest.csv`. \n",
    "\n",
    "Here is a brief description of the features in the data set:\n",
    "1. <b>pre-rx-cost</b>: Total pharmacy costs per person\n",
    "2. <b>numofgen</b>: Number of generic scripts\n",
    "3. <b>numofbrand</b>: Number of brand scripts\n",
    "4. <b>generic-cost</b>: Cost of generic scripts filled\n",
    "5. <b>adjust-total-30d</b>: 30 day adjusted fill rate\n",
    "6. <b>num-er</b>: Number of ER visits\n",
    "7. <b>region</b>: US Census Region (1 = Northeast, 2 = Midwest, 3 = South, 4 = West)\n",
    "8. <b>pdc-80-flag</b>: Adherent (A categorical variable that indicates if patients have adhered to taking their medications more than 80% of the time; = 1 if pdc â‰¥ 0.80; = 0 otherwise)\n",
    "\n",
    "We will be using the following features to train our model:\n",
    "* pre_rx_cost<br>\n",
    "<li>numofgen<br>\n",
    "<li>numofbrand<br>\n",
    "<li>generic-cost<br>\n",
    "<li>adjust_total_30d<br>\n",
    "<li>num_er<br>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/healthcare_train.csv', index_col=0)\n",
    "test_df = pd.read_csv('./data/healthcare_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need this helper function\n",
    "def normalize(a, b):\n",
    "    r\"\"\"\n",
    "    This does not really handle bad parameters - expects Pandas Series for both a and b\n",
    "    a : 'target' DF which includes the specific values we want to normalize\n",
    "    b : the 'master' DF which includes all possible values in the space (including a)\n",
    "    \"\"\"\n",
    "    return ([(a.iloc[i] - b.min()) / (b.max() - b.min())\n",
    "             for i in range(len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "    'pre_rx_cost', 'numofgen', 'numofbrand', 'generic_cost',\n",
    "    'adjust_total_30d', 'num_er'\n",
    "]\n",
    "\n",
    "# training data\n",
    "input_train_labels = train_df[['pdc_80_flag']]\n",
    "input_train_features = train_df[feature_list]\n",
    "\n",
    "# testing data\n",
    "input_test_labels = test_df[['pdc_80_flag']]\n",
    "input_test_features = test_df[feature_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>We need to normalize our data. Normalizing is an important step for KNN (and in many other cases) because it prevents one field from overwhelming the others. For example, if one feature's values rang from 0.002 to 0.006 (i.e. some delicate measurement) awhereas another feature's values range from 800 to 1600, we cannot weight them equally when calculating our KNN algorithm.\n",
    "\n",
    "Normalizing (in this case) is taking the features (columns) and re-scaling them equally, for example squeezing the minimum to be 0 and the maximum to be 1, and redistributing all of the data in between that range for each feature. The proportion between the data points in each feature is preserved, which is what matters.<br><br>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Normalization_(statistics)\n",
    "\n",
    "There is a bit of a trick here- we have to combine our ***test data*** and our ***train data*** and normalize them together. Otherwise our training data set will potentially be scaled incorrectly.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_features_norm = input_train_features.copy()\n",
    "input_test_features_norm = input_test_features.copy()\n",
    "\n",
    "merge = [input_train_features, input_test_features]\n",
    "\n",
    "# use this merged df as a basis for the span of values in each column\n",
    "merge_df = pd.concat(merge)\n",
    "\n",
    "for col in range(input_train_features_norm.shape[1]):\n",
    "    # normalize each column of train values\n",
    "    input_train_features_norm.iloc[:, col] = normalize(\n",
    "        input_train_features.iloc[:, col], merge_df.iloc[:, col])\n",
    "\n",
    "    # normalize each column of test values\n",
    "    input_test_features_norm.iloc[:, col] = normalize(\n",
    "        input_test_features.iloc[:, col], merge_df.iloc[:, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>Let's build our model. We are going to step through a variety of $k$ values and figure out which $k$ provides the best overall accuracy (this is the ***training*** phase).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing variables\n",
    "# you can change the two k values if you want!\n",
    "\n",
    "k_init = 75\n",
    "k_max = 105\n",
    "\n",
    "k_stepsize = 2\n",
    "n_steps = int((k_max - k_init) / k_stepsize + 1)\n",
    "k_values = [k_init + i * k_stepsize for i in range(n_steps)]\n",
    "\n",
    "n_predictions = input_test_features_norm.shape[0]\n",
    "\n",
    "predictions = np.zeros([n_steps, n_predictions])\n",
    "\n",
    "# set initial value of predicted PDC80 to -1, an invalid value\n",
    "predictions[:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn model\n",
    "\n",
    "for i in range(n_steps):\n",
    "    k = i * k_stepsize + k_init\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=k, weights='uniform', p=2, metric='minkowski')\n",
    "    model.fit(input_train_features_norm, input_train_labels.values.ravel())\n",
    "\n",
    "    # store predictions for this k\n",
    "    predictions[i, :] = model.predict(input_test_features_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>Now we can evaluate our model. This is the ***testing*** phase.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our success rate\n",
    "\n",
    "prediction_rate = np.zeros([n_steps])\n",
    "\n",
    "for i in range(n_steps):\n",
    "    prediction_rate[i] = sum([\n",
    "        predictions[i, j] == input_test_labels.iloc[j]\n",
    "        for j in range(n_predictions)\n",
    "    ]) / n_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "plt.plot(prediction_rate)\n",
    "plt.title('$k$NN Accuracy for $k$=' + str(k_init) + ' through $k$=' + str(k_max))\n",
    "plt.xticks(np.arange(n_steps), k_values)\n",
    "plt.xlabel('$k$')\n",
    "plt.plot(13.985, .67733, 'ro')\n",
    "plt.hlines(y=np.mean(prediction_rate), xmin=0, xmax=n_steps-1, linestyles=':', colors='green')\n",
    "plt.text(14.1, .67733, 'max')\n",
    "plt.text(14, np.mean(prediction_rate)*1.001, 'mean='+str(round(np.mean(prediction_rate),4)))\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>The values for $k$ which produced the best results was $k = 103$, with an accuracy of 67.73%.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#D21087\">$k$-Means Clustering</font>\n",
    "\n",
    "<font color='#544640'>\n",
    "\n",
    ">$k$-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. $k$-means clustering aims to partition n observations into $k$ clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into ***Voronoi cells***.\n",
    "\n",
    "https://en.wikipedia.org/wiki/K-means_clustering<br><br><br>\n",
    "\n",
    "\n",
    "<center><img src='./img/kmeans1.gif' width=300 alt='K-Means><br></center>\n",
    "\n",
    "<center><img src='./img/kmeans2.gif' width=400 alt='K-Means'><br></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In mathematics, a Voronoi diagram is a partitioning of a plane into regions based on distance to points in a specific subset of the plane. That set of points (called seeds, sites, or generators) is specified beforehand, and for each seed there is a corresponding region consisting of all points closer to that seed than to any other. These regions are called Voronoi cells.<br> \n",
    "> <br>In the simplest case, shown in the first picture, we are given a finite set of points $\\{~p_1,~\\ldots,~p_n\\}$ in the Euclidean plane. In this case each site pk is simply a point, and its corresponding Voronoi cell $R_k$ consists of every point in the Euclidean plane whose distance to $p_k$ is less than or equal to its distance to any other $p_k$. Each such cell is obtained from the intersection of half-spaces, and hence it is a convex polygon. The line segments of the Voronoi diagram are all the points in the plane that are equidistant to the two nearest sites. The Voronoi vertices (nodes) are the points equidistant to three (or more) sites.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Voronoi_diagram<br><br>\n",
    "\n",
    "An example of a Voronoi Diagram:<br><br>\n",
    "\n",
    "<center><img src='./img/voronoi.png' width=300 alt='Voronoi Diagram'><br></center>\n",
    "\n",
    "<center><font size=1>By <a href='https://commons.wikimedia.org/w/index.php?curid=38534275'>Balu Ertl - Own work</a>, CC BY-SA 4.0 </font></center><br><br>\n",
    "\n",
    "We use $k$-means for clustering in this example, but in other cases this algorithm can help with feature learning and signal processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>Say we have a set of data regarding visitors to our website and we'd like to classify them by their behavior. We could know which pages they visit when, for example, how long they stay on the site, how frequently they visit, etc.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitors_df = pd.read_csv('./data/visitors.csv', index_col = 0)\n",
    "#visitors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>For this example we have three dimensions. Conveniently, we also live in and observe the world in three dimensions, so we can do a pretty fancy plot.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "colormap = np.array(['yellow', 'red', 'blue','green','magenta'])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(xs = visitors_df.homepage, \n",
    "           ys = visitors_df.about_us_page, \n",
    "           zs = visitors_df.products_page, \n",
    "           s= 180, \n",
    "           depthshade = False)\n",
    "    \n",
    "ax.set_xlabel('Homepage')\n",
    "ax.set_ylabel('About Us')\n",
    "ax.set_zlabel('Products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>Click and drag the plot to rotate.\n",
    "\n",
    "We'll create a dendrogram to analyze the way these visitors are similar, and how they break down into similar or dissimilar groups. This helps us to understand how many clusters we want to try to form, $k=1,2,3\\ldots$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(visitors_df, 'average')\n",
    "plt.figure(figsize=(10, 10))\n",
    "D = dendrogram(\n",
    "    Z=Z,\n",
    "    orientation='bottom', \n",
    "    color_threshold=7.5,\n",
    "    leaf_font_size=11, \n",
    "    labels = visitors_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>We will choose to go with $k=4$ clusters for our model. Once the data points are clustered, we'll replot the dataset with the points colored to represent the cluster they were found to exist in.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "model = KMeans(n_clusters = k)\n",
    "model.fit(visitors_df[['homepage','about_us_page','products_page']])\n",
    "# print(model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "colormap = np.array(['yellow', 'red', 'blue','green','magenta'])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(xs = visitors_df.homepage, \n",
    "           ys = visitors_df.about_us_page, \n",
    "           zs = visitors_df.products_page, \n",
    "           s= 180, \n",
    "           depthshade = False, \n",
    "           c = colormap[model.labels_])\n",
    "    \n",
    "ax.set_xlabel('Homepage')\n",
    "ax.set_ylabel('About Us')\n",
    "ax.set_zlabel('Products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>Rotating the plot around will show you how we have clustered the users into four groups. The algorithm did a pretty good job. You can almost imagine a three-dimensional (but invisible) ***Voronoi diagram*** separating the points.\n",
    "\n",
    "Now that we have this model built, we can use it to predict a future user given their usage information. Try it yourself.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Simulate a website user. Enter the user stats below..')\n",
    "try:\n",
    "    homepage = float(input('  Hits on Homepage  [1 - 15]: '))\n",
    "    about_us = float(input('  Hits in About Us? [1 - 20]: '))\n",
    "    products = float(input('  Hits in Products  [1 - 20]: '))\n",
    "except:\n",
    "    print('Something went wrong.')\n",
    "      \n",
    "data_class = model.predict(np.array([homepage, about_us, products]).reshape(1, -1))[0]\n",
    "\n",
    "print('\\nThe predicted category for this user data point is ' + str(colormap[data_class]))\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "colormap = np.array(['yellow', 'red', 'blue','green','magenta'])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(xs = visitors_df.homepage, \n",
    "           ys = visitors_df.about_us_page, \n",
    "           zs = visitors_df.products_page, \n",
    "           s= 180, \n",
    "           depthshade = False, \n",
    "           c = colormap[model.labels_])\n",
    "\n",
    "ax.plot([homepage], [about_us], [products], markerfacecolor='k', markeredgecolor='k', marker='x', markersize=15, alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('Homepage')\n",
    "ax.set_ylabel('About Us')\n",
    "ax.set_zlabel('Products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>What happens when you put a much larger number into the model for one of its inputs?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
