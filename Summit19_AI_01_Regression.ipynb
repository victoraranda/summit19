{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/insight.svg\" style=\"width: 300px\"><br>\n",
    "<font color='#544640'>\n",
    "<center><i>Engineering Summit 2019</i></center>\n",
    "<center><i>Denver, Colorado</i></center></font><br>\n",
    "<center><i><font color='#544640' size='1'>Author: Victor Aranda</font></i></center></font>\n",
    "<center><i><font color='#B81590' size='1'>victor.aranda@insight.com</font></i></center></font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#D21087\">Regression</font>\n",
    "\n",
    "<font color='#544640'>\n",
    "\n",
    "> In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.<br>\n",
    "> <br>Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.<br>\n",
    "> <br>Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Linear_regression\n",
    "\n",
    "Often times ***linear regression***, despite its weaknesses, is the best fit to a problem. Although a powerful, trained neural network or an ensemble solution may provide better accuracy, it could take months or years to build. Linear regression often produces a 'good enough' result with minimal effort.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import scipy.linalg\n",
    "from scipy.optimize import minimize\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#D21087\">Two Dimensions</font>\n",
    "\n",
    "<font color='#544640'>Can you imagine the line that predicts (or follows) the arrangement of these data points?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x1 = 10 * rng.rand(50)\n",
    "y1 = 2 * x1 - 5 + rng.randn(50)\n",
    "\n",
    "plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "plt.scatter(x1, y1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"#D21087\">Two Points</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>The simplest possible case of ***linear regression*** is a set of only two points, in two dimensions. In this case, our model is simply a Least-Squares solution to an $A x = b$ linear system; the equation of a line.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x2 = np.array([1, 3])\n",
    "y2 = np.array([3, 5])\n",
    "\n",
    "plt.subplots(1, 1, figsize=(8, 8))\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(0, 6)\n",
    "plt.plot(x2, y2, \"o\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the two (*known*) points: $(x_1, y_1) = (1,3) $ and $(x_2, y_2) = (3,5) $, we must solve the equations:\n",
    "\n",
    "$c_0 + c_1 x_1 = y_1$  \n",
    "$c_0 + c_1 x_2 = y_2$  \n",
    "\n",
    "$\\displaystyle{\n",
    "\\left(\\begin{matrix} 1 & x_1\\\\ 1 & x_2\\end{matrix}\\right)\\left(\\begin{matrix}c_0 \\\\ c_1\\end{matrix}\\right) = \n",
    "\\left(\\begin{matrix} y_1 \\\\ y_2\\end{matrix}\\right)\n",
    "}$\n",
    "\n",
    "for $c_0$ and $c_1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "D = np.array([[1, 1], x2.T])\n",
    "\n",
    "# it's like cheating!\n",
    "c0, c1 = np.linalg.solve(D, y2)\n",
    "\n",
    "plt.subplots(1, 1, figsize=(8, 8))\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(0, 6)\n",
    "plt.plot(x2, y2, 'ob')\n",
    "\n",
    "xw = np.linspace(0, 5, 6)\n",
    "\n",
    "plt.plot(xw, c0 + c1 * xw, '-r', lw=.5)\n",
    "plt.text(3, 4, r'y = ${:.2f} + {:.2f}x$'.format(c0, c1), fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"#D21087\">Three Points</font>\n",
    "\n",
    "<font color='#544640'>Once a third point is added, the line may no longer pass through the points. It will still find the 'best fit', and we're still solving a system of linear equations. We can still define the best linear solution as the one which exhibits the least 'error' or which minimizes 'residuals'. There are several ways to calculate these fits.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = np.array([[1, 3], [3, 5], [5, 5]])\n",
    "\n",
    "x3 = np.array([1, 3, 5])\n",
    "y3 = np.array([3, 3, 5])\n",
    "\n",
    "plt.subplots(1, 1, figsize=(8, 8))\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(2, 6)\n",
    "plt.plot(x3, y3, 'o')\n",
    "\n",
    "c1, c0 = np.polyfit(x3, y3, 1)\n",
    "\n",
    "plt.text(1, 4, 'y = {:.2f} + {:.2f}x'.format(c0, c1), fontsize=18)\n",
    "\n",
    "xw = np.linspace(0, 10, 11)\n",
    "plt.plot(xw, c0 + c1 * xw, '-r', lw=.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"#D21087\">Polynomial Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>What happens when the data isn't linear? How do we fit? Here we will create a data set based on the equation:\n",
    "\n",
    "$$ \\Large y = \\frac{1}{2}x^3 -2x^2 + x$$\n",
    "\n",
    "We will also add some noise to the data to make it look a little bit more realistic.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from\n",
    "# https://towardsdatascience.com/polynomial-regression-bbe8b9d97491\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x4 = np.sort(2 - 3 * np.random.normal(0, 2, 60))\n",
    "y4 = x4 - 2 * (x4**2) + 0.5 * (x4**3) + np.random.normal(-100, 100, 60)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlim(-15, 20)\n",
    "plt.ylim(-1000, 2000)\n",
    "plt.title('Suspiciously Nonlinear set of data')\n",
    "\n",
    "plt.scatter(x4, y4, s=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#544640'>Let's create and plot a linear fit.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x5 = x4[:, np.newaxis]\n",
    "y5 = y4[:, np.newaxis]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x5, y5)\n",
    "y_linear_pred = model.predict(x5)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.xlim(-15, 20)\n",
    "plt.ylim(-1000, 2000)\n",
    "plt.title('Suspiciously Nonlinear Set of Data\\nLinear Fit')\n",
    "\n",
    "plt.scatter(x5, y5, s=10)\n",
    "plt.plot(x5, y_linear_pred, '-r', lw=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>\n",
    "Does the linear model do a 'good enough' job? Are the points in the data set adequately described by the line? What is the rationale behind your answer?<br><br>\n",
    "\n",
    "<center><img src='./img/over_line.gif' width=500></center><br><br>\n",
    "\n",
    "Damnit, Walter, now we need to fit a 3rd-order polynomial ($x^3$) for this set of points, man.</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=3)\n",
    "x_poly = polynomial_features.fit_transform(x5)\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y5)\n",
    "\n",
    "y_poly_pred = model.predict(x_poly)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y5, y_poly_pred))\n",
    "r2 = r2_score(y5, y_poly_pred)\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlim(-15, 20)\n",
    "plt.ylim(-1000, 2000)\n",
    "plt.title('Suspiciously Nonlinear Set of Data\\nPoly Fit (3)')\n",
    "\n",
    "plt.scatter(x5, y5, s=10)\n",
    "\n",
    "# # sort the values of x before line plot\n",
    "# sort_axis = operator.itemgetter(0)\n",
    "\n",
    "#sorted_zip_poly = zip(x5, y_poly_pred)\n",
    "#x5, y_poly_pred = zip(*sorted_zip_poly)\n",
    "\n",
    "plt.plot(x5, y_poly_pred, '-r', lw=.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>Our polynomial ($x^3$) model fits the data much more closely. We will now take a look at the residuals for the two models to compare their accuracy - specifically how that accuracy varies along $x$, which is a key consideration.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total residuals for both models\n",
    "\n",
    "resid_linear = np.abs(y5 - y_linear_pred)\n",
    "resid_poly = np.abs(y5 - y_poly_pred)\n",
    "\n",
    "# plot the residuals over x for both models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x5, resid_linear, label='Linear Residuals')\n",
    "plt.plot(x5, resid_poly, label='Polynomial (x^3) Residuals')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('$x$ values')\n",
    "plt.ylabel('$y$_actual  -  $y$_predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Total residual for linear fit:',round(np.sum(resid_linear),2))\n",
    "print('Total residual for polynomial fit:',round(np.sum(resid_poly),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#544640'>Which is 'better'? What is the rationale for your answer?</font><br><br>\n",
    "\n",
    "<center><img src='./img/will_not_stand.gif' width=500></center><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"#D21087\">Overfitting</font>\n",
    "\n",
    "<font color='#544640'>Nonlinear fits can capture the behavior of many natural systems. Careful - the higher the order, the more likely we are to over fit. Depending on the use case, we sometimes want a model that will be generally applicable to new, unseen data from the same population - this comes up in classification. If the model fits too 'tightly', its error versus observed data will be higher than a 'looser' fit.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=20)\n",
    "x_poly = polynomial_features.fit_transform(x5)\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y5)\n",
    "\n",
    "y_poly_pred = model.predict(x_poly)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y5, y_poly_pred))\n",
    "r2 = r2_score(y5, y_poly_pred)\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlim(-15, 20)\n",
    "plt.ylim(-1000, 2000)\n",
    "plt.title('Suspiciously Nonlinear Set of Data\\nPolynomial Fit (20)')\n",
    "\n",
    "plt.scatter(x5, y5, s=10)\n",
    "\n",
    "# # sort the values of x before line plot\n",
    "# sort_axis = operator.itemgetter(0)\n",
    "\n",
    "#sorted_zip_poly = zip(x5, y_poly_pred)\n",
    "#x5, y_poly_pred = zip(*sorted_zip_poly)\n",
    "\n",
    "plt.plot(x5, y_poly_pred, '-r', lw=.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"#D21087\">Anscombe's Quartet</font>\n",
    "\n",
    "<font color='#544640'>\n",
    "\n",
    "> Anscombe's quartet comprises four datasets that have nearly identical simple descriptive statistics, yet appear very different when graphed. Each dataset consists of eleven (x,y) points. They were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data before analyzing it and the effect of outliers on statistical properties. He described the article as being intended to counter the impression among statisticians that \"numerical calculations are exact, but graphs are rough.\"\n",
    "\n",
    "https://en.wikipedia.org/wiki/Anscombe%27s_quartet\n",
    "<center><img src='./img/anscombe_quartet.png'></center><br><br><br><br>\n",
    "\n",
    "For all four datasets share the exact same summary statistics. On the surface, they're indistinguishable!<br>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Property</th>\n",
    "        <th>Value</th>\n",
    "        <th>Accuracy</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Mean of $x$</td>\n",
    "        <td>9</td>\n",
    "        <td>exact</td>\n",
    "    </tr>\n",
    "  <tr>\n",
    "        <td>Sample variance of $x$</td>\n",
    "        <td>11</td>\n",
    "        <td>exact</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Mean of $y$</td>\n",
    "        <td>7.50</td>\n",
    "        <td>to 2 decimal places</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Sample variance of $y$</td>\n",
    "        <td>4.125</td>  \n",
    "        <td>±0.003</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Correlation between $x$ and $y$</td>\n",
    "        <td>0.816</td>\n",
    "        <td>to 3 decimal places</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Linear regression line</td>\n",
    "        <td>$y = 3.00 + 0.500x$</td>\n",
    "        <td>to 2 and 3 decimal places, respectively</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Coefficient of determination<br>of the linear regression</td>\n",
    "        <td>0.67</td>\n",
    "        <td>to 2 decimal places</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#D21087\">Three Dimensions</font>\n",
    "\n",
    "<font color='#544640'>Imagine a three-dimensional space filled with data points. These data points may appear to fall into a single plane, or appear as one or more clusters, or follow some other distribution. The same principles we have been discussion apply, althogh the mathematics gets a bit more difficult.\n",
    "\n",
    "In 2D we fit a ***line*** - in 3D we fit a ***plane***. In data sets of $n$ dimensions (which are difficult to visualize), we fit ***hyperplanes***.\n",
    "\n",
    ">In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines. This notion can be used in any general space in which the concept of the dimension of a subspace is defined. In machine learning, hyperplanes are a key tool to create support vector machines for such tasks as computer vision and natural language processing.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Hyperplane\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helpers\n",
    "\n",
    "def fit(X, params):\n",
    "    # 3d Plane Z = aX + bY + c\n",
    "    return X.dot(params[:2]) + params[2]\n",
    "\n",
    "def cost_function(params, X, y):\n",
    "    # L1- norm\n",
    "    return np.sum(np.abs(y - fit(X, params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "mean = np.array([0.0, 0.0, 0.0])\n",
    "cov = np.array([[1.0, -0.5, 0.8], [-0.5, 1.1, 0.0], [0.8, 0.0, 1.0]])\n",
    "data = np.random.multivariate_normal(mean, cov, 50)  # some 3-dim points\n",
    "mean = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "output = minimize(\n",
    "    cost_function, [0.5, 0.5, 0.5],\n",
    "    args=(np.c_[data[:, 0], data[:, 1]], data[:, 2]))\n",
    "y_hat = fit(np.c_[data[:, 0], data[:, 1]], output.x)\n",
    "\n",
    "X, Y = np.meshgrid(\n",
    "    np.arange(min(data[:, 0]), max(data[:, 0]), 0.5),\n",
    "    np.arange(min(data[:, 1]), max(data[:, 1]), 0.5))\n",
    "XX = X.flatten()\n",
    "YY = Y.flatten()\n",
    "\n",
    "Z = output.x[0] * X + output.x[1] * Y + output.x[2]\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, alpha=0.2)\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='./img/amazing.gif' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font color='#544640'>Minds = blown!</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"#B81590\">$$\\large-\\infty-$$</font><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
